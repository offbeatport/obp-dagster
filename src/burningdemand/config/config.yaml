raw_gh_issues:
  queries_per_day: 4 # Number of queries to make per day (each query is a day split into queries_per_day hours)
  max_parallel: 2 # Max number of GitHub range queries to run concurrently
  min_reactions: 1 # Only return issues with at least this many reactions
  min_comments: 1 # Only return issues with at least this many comments
  max_comments: 20 # Number of comments to fetch per issue
  max_labels: 20 # Number of labels to fetch per issue
  per_page: 50 # Number of issues to fetch per request

raw_gh_discussions:
  queries_per_day: 3 # Number of queries to make per day (each query is a day split into queries_per_day hours)
  max_parallel: 2 # Max number of GitHub range queries to run concurrently
  min_comments: 1 # Only return discussions with at least this many comments
  max_labels: 20 # Number of comments to fetch per discussion
  max_comments: 20 # Number of comments to fetch per discussion
  per_page: 50 # Number of discussions to fetch per request

raw_gh_pull_requests:
  queries_per_day: 4 # Number of queries to make per day (each query is a day split into queries_per_day hours)
  max_parallel: 2 # Max number of GitHub range queries to run concurrently
  min_comments: 1 # Only return pull requests with at least this many comments
  max_labels: 10 # Number of labels to fetch per pull request
  max_comments: 10 # Number of comments to fetch per pull request
  per_page: 50 # Number of pull requests to fetch per request

issues:
  llm_concurrency_per_issues_partition: 8
  # How much cluster content we send to the LLM (labeling)
  labeling:
    max_representatives_for_labeling: 10
    max_snippets_for_labeling: 5
    max_body_length_for_snippet: 500
  # LLM for cluster labeling (can be overridden with LLM_MODEL env var)
  llm:
    model: "gpt-5-mini"
    base_url: "https://llm.offbeatport.com"
    max_tokens: 2000

embeddings:
  # Default embedding model (override with EMBEDDING_MODEL env var)
  # model: "all-MiniLM-L6-v2"
  model: "BAAI/bge-large-en-v1.5"

  # Batch size when calling the embedding model (SentenceTransformer.encode)
  encode_batch_size: 16
  # Batch size for splitting items in the embeddings asset
  asset_batch_size: 1000

clustering:
  rolling_window_days: 14
  min_cluster_size_floor: 5
  min_cluster_size_divisor: 150
  min_samples_floor: 1
  min_samples_divisor: 4
  metric: "euclidean"
  selection_method: "eom"
  representatives:
    diversity_threshold: 0.8
